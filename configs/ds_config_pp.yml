train_micro_batch_size_per_gpu: 1
train_batch_size: 128
dataloader_drop_last: true
steps_per_print: 20
optimizer: 
  type: AdamW
  params: 
    lr: 1.0e-5
    betas: [0.9, 0.999]
    weight_decay: 4.0e-5
    eps: 1.0e-8
  no_wd_kws: ['bias', 'norm'] # remove this or empty list if no need of this
scheduler: 
  type: WarmupCosineLR
  params: 
    warmup_num_steps: 100
    warmup_min_ratio: 0.02
    cos_min_ratio: 0.2 

bf16: 
  enabled: false 
fp16: 
  enabled: true
  auto_cast: false
  fused_optimizer: false
  loss_scale: 0
  initial_scale_power: 16
  loss_scale_window: 1000
  hysteresis: 2
  min_loss_scale: 1

zero_allow_untested_optimizer: true

aux_max_z_loss: # remove this if you don't need this aux loss
    weight: 2.0e-4

model_topo: 
  process_topology: 
    axes: [pipe, data]
    dims: [8, 1]
  parts: [1, 5, 5, 5, 5, 5, 5, 1]  # bloom-7b
  # parts: [5, 4, 4, 4, 4, 4, 4, 5]  # llama-7b

from_scratch: false
model_path: ./saved_bloomz_7b1_mt_pp/
save_path: ./checkpoints/

n_epoches: 3
max_seq_len: 1024
data_path: ./dataset.json

use_grad_ckpt: false

use_flash_attn: false
